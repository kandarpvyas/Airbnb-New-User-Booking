{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Neural Network\n",
    "\n",
    "This notebook demonstrates the entire process of building a predictive model using Keras sequential model to suggest the first destination of new Airbnb Users. All the processes involved, such as data wrangling, exploratory data analysis, inferential statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load all the data available to us in a Pandas Dataframe and extract basic information such as number of samples, number of null values rows, number of features, etc. Here I have used Keras and Tensorflow as a backend.\n",
    "\n",
    "The next step would be to deal with the missing values using a suitable method (dropping, interpolating, etc.) and convert certain features into a more suitable form for applying inferential statistics and machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNA(df):\n",
    "    df = df.replace(r'\\s+', np.nan, regex=True)\n",
    "    df = df.replace('-unknown-',np.nan, regex=False)\n",
    "    df = df.replace('Other/Unknown',np.nan, regex=False)\n",
    "    df = df.dropna(thresh=10) #Ignore the rows with majority Missing Value during Analysis\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will remove '-unknown-' and 'Other/Unknown' values from the CSV file and return cleaned data frame.\n",
    "Here I have set thresh value to 10. It means that in a single row at least 10 N/A values are allowed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDate(df):\n",
    "    df['date_account_created']=pd.to_datetime(df['date_account_created']).dt.dayofweek\n",
    "    df['date_first_booking']=pd.to_datetime(df['date_first_booking']).dt.dayofweek\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encodeDate function will extract day of week from date_account_created and date_first_booking columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedRandomImputation(df):\n",
    "    for col in df:\n",
    "        nan_count=df[col].isnull().sum()\n",
    "        #print(\"nan_count=:::\"+nan_count)\n",
    "        if col=='age':\n",
    "            df=handleOutlierAge(df)\n",
    "            \n",
    "        # For parameters other then age, compute their missing value using stratified methodology of missing value imputation    \n",
    "        if nan_count>0 and col!='age': \n",
    "            df_counts=df[col].value_counts() #Count columnwise repeated value\n",
    "            Total_minus_unknown = 0\n",
    "            Total_minus_unknown = len(df[col]) - len(df_counts) #Get unknown value count in a column\n",
    "            ratio_list=[]\n",
    "            for i in range(len(df_counts)):\n",
    "                ratio_list.append(float(df_counts[i])*100/float(Total_minus_unknown))  #Multiplying actual value with 100 and diviving it with unknown value count\n",
    "            min_ratio = min(ratio_list)  #Finding minimum value from ration_List\n",
    "            ratio_list = [int(x/min_ratio) for x in ratio_list] #Divide actual value with min_ratio\n",
    "            counts_list=df_counts.index.tolist() #Convert Weight to list\n",
    "            pairs = list(zip(ratio_list,counts_list)) #merge Weight with new min_ratio value\n",
    "            df[col]=df[col].apply(lambda x: weightedRandomHelper(pairs) if(pd.isnull(x)) else x)\n",
    "            # Creating bins for signup_flow parameter\n",
    "        if col=='signup_flow': \n",
    "            bins = [-1,5,10,15,20,28]\n",
    "            group_names = [0,1,2,3,4]\n",
    "            df['signup_flow_bins'] = pd.cut(df['signup_flow'], bins, labels=group_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function weightedRandomImputation() takes data frame as an argument and removes outliers from age column.\n",
    "For the columns other than age , if N/A value count is greater than zero then it will identify those values and replace it with the mean values.\n",
    "\n",
    "Here I have created bins for signup_flow column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedRandomHelper(pairs):  \n",
    "    total = sum(pair[0] for pair in pairs)\n",
    "    r = randint(1, total)\n",
    "    for (weight, value) in pairs:\n",
    "        r -= weight\n",
    "        if r <= 0: return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleOutlierAge(df):\n",
    "    df['age']=df['age'].apply(lambda x: datetime.now().year-x if x>1900 else x)\n",
    "    \n",
    "    #Valid age range between 14 to 90 as per data, otherwise check if its outlier or not\n",
    "    df['age']=df['age'].apply(lambda x: x if 14<=x<=90 else np.nan)     \n",
    "    mean = df['age'].mean()\n",
    "    mean = int(mean)\n",
    "    df['age']=df['age'].apply(lambda x: mean if np.isnan(x) else x) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will remove outliers from age column.Here I have taken valid age range is between 14 to 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Preprocessing\n",
      "Handling Missing Values\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train_users_2.csv')   #load data\n",
    "\n",
    "print(\"Doing Preprocessing\")\n",
    "print(\"Handling Missing Values\")\n",
    "df = findNA(df)\n",
    "original_data  = df.copy()\n",
    "original_data=encodeDate(original_data)   #convert date to the day of the week with Monday=0, Sunday=6\n",
    "original_data=weightedRandomImputation(original_data) # Missing Value Imputation\n",
    "\n",
    "df,df_test = train_test_split( df, test_size=0.3, stratify=df['country_destination'])\n",
    "\n",
    "#This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n",
    "#Here country_destination is a categorial variable\n",
    "\n",
    "\n",
    "df=encodeDate(df)   #convert date to the day of the week with Monday=0, Sunday=6\n",
    "df=weightedRandomImputation(df) # Missing Value Imputation\n",
    "\n",
    "#preprocess of test\n",
    "df_test = encodeDate(df_test)\n",
    "df_test = weightedRandomImputation(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(df,df_test):\n",
    "\n",
    "    print(\"\\nLearning the Keras Neural Network Classifier Model...\")\n",
    "    Y_train = df.country_destination\n",
    "    X_train = df.drop('country_destination', 1)\n",
    "    X_train = X_train.drop('id', 1)\n",
    "\n",
    "    #preprocess of test\n",
    "    Y_test = df_test.country_destination\n",
    "    X_test = df_test.drop('country_destination', 1)\n",
    "    X_test = X_test.drop('id', 1)\n",
    "\n",
    "    # encode Y train\n",
    "    le = LabelEncoder()\n",
    "    Y_train = le.fit_transform(Y_train)\n",
    "\n",
    "    # Encode Y Test \n",
    "    le_t = LabelEncoder()\n",
    "    Y_test = le_t.fit_transform(Y_test)\n",
    "\n",
    "    #dropping columns as they dont improve accuracy\n",
    "    X_train = X_train.drop('timestamp_first_active', 1)\n",
    "    X_train = X_train.drop('language', 1)\n",
    "    X_train = X_train.drop('signup_app', 1)\n",
    "    \n",
    "    X_test = X_test.drop('timestamp_first_active', 1)\n",
    "    X_test = X_test.drop('language', 1)\n",
    "    X_test = X_test.drop('signup_app', 1)\n",
    "\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y_train)\n",
    "    encoded_Y = encoder.transform(Y_train)\n",
    "    #print(encoded_Y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_train = pd.DataFrame(np_utils.to_categorical(encoded_Y)) #Convert class vector (integers from 0 to nb_classes) to binary class matrix, for use with categorical_crossentropy.\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y_test)\n",
    "    encoded_Y = encoder.transform(Y_test)\n",
    "    #print(encoded_Y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    Y_test = pd.DataFrame(np_utils.to_categorical(encoded_Y))\n",
    "    df_encoded = pd.DataFrame(index=range(1,len(X_train))) #Store index value from 1 to len(X_train)\n",
    "    train = pd.concat([X_train, X_test])\n",
    "    \n",
    "    for col in train:\n",
    "        if col=='age': \n",
    "            bins = [13,20,30,40,50,60,70,80,91]\n",
    "            group_names = [0,1,2,3,4,5,6,7]\n",
    "            train['age_bins'] = pd.cut(train['age'], bins, labels=group_names)\n",
    "            train=train.drop('age',1)\n",
    "            col = 'age_bins'\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train[col])\n",
    "        encoded_Col = encoder.transform(train[col]) #Convert into categorial data\n",
    "        df_encoded = pd.concat([df_encoded,pd.DataFrame(np_utils.to_categorical(encoded_Col))],axis=1)\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=len(df_encoded.columns), activation='relu')) #12 is the dimension of the output space.\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(df_encoded.values[:len(X_train)], Y_train.values, epochs=30, batch_size=1000)\n",
    "    scores = model.evaluate(df_encoded.values[:len(X_train)], Y_train.values)\n",
    "    print(\"\\nTraining Score: %.2f\" % (scores[1]*100))\n",
    "    scores = model.evaluate(df_encoded.values[len(X_train):], Y_test.values)\n",
    "    print(\"\\nTesting Score: %.2f\" % (scores[1]*100))\n",
    "    model.summary()\n",
    "    Y_pred = model.predict(df_encoded.values[len(X_train):])\n",
    "    print(\"The confusion matrix is : \\n\",confusion_matrix(Y_test.values.argmax(axis=1), Y_pred.argmax(axis=1)))\n",
    "    #print(\"Mean Absolute error is :\",mean_absolute_error(Y_test.values.argmax(axis=1), Y_pred.argmax(axis=1)))\n",
    "    #print(\"Evaluation Metrics : \\n\",classification_report(Y_test.values.argmax(axis=1), Y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have used Sequential Keras model.There are three layers and in each layer activation function is ReLu.\n",
    "\n",
    "By using Cross-Entropy cost and ADAM optimizer , model has achieved training score and testing score accuracy 58.35% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning the Keras Neural Network Classifier Model...\n",
      "Epoch 1/30\n",
      "149415/149415 [==============================] - 4s 27us/step - loss: 2.4769 - acc: 0.3112\n",
      "Epoch 2/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.5237 - acc: 0.5806\n",
      "Epoch 3/30\n",
      "149415/149415 [==============================] - 3s 20us/step - loss: 1.4354 - acc: 0.5835\n",
      "Epoch 4/30\n",
      "149415/149415 [==============================] - 3s 20us/step - loss: 1.4051 - acc: 0.5835\n",
      "Epoch 5/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3799 - acc: 0.5835\n",
      "Epoch 6/30\n",
      "149415/149415 [==============================] - 4s 24us/step - loss: 1.3791 - acc: 0.5835\n",
      "Epoch 7/30\n",
      "149415/149415 [==============================] - 3s 23us/step - loss: 1.3620 - acc: 0.5835\n",
      "Epoch 8/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3754 - acc: 0.5835\n",
      "Epoch 9/30\n",
      "149415/149415 [==============================] - 3s 23us/step - loss: 1.3899 - acc: 0.5835\n",
      "Epoch 10/30\n",
      "149415/149415 [==============================] - 4s 24us/step - loss: 1.3668 - acc: 0.5835\n",
      "Epoch 11/30\n",
      "149415/149415 [==============================] - 4s 25us/step - loss: 1.3568 - acc: 0.5835\n",
      "Epoch 12/30\n",
      "149415/149415 [==============================] - 4s 25us/step - loss: 1.3490 - acc: 0.5835\n",
      "Epoch 13/30\n",
      "149415/149415 [==============================] - 4s 24us/step - loss: 1.3483 - acc: 0.5835\n",
      "Epoch 14/30\n",
      "149415/149415 [==============================] - 4s 28us/step - loss: 1.3413 - acc: 0.5835\n",
      "Epoch 15/30\n",
      "149415/149415 [==============================] - 4s 28us/step - loss: 1.3405 - acc: 0.5835\n",
      "Epoch 16/30\n",
      "149415/149415 [==============================] - 3s 23us/step - loss: 1.3371 - acc: 0.5835\n",
      "Epoch 17/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3311 - acc: 0.5835\n",
      "Epoch 18/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3363 - acc: 0.5835\n",
      "Epoch 19/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3253 - acc: 0.5835\n",
      "Epoch 20/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3216 - acc: 0.5835\n",
      "Epoch 21/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3220 - acc: 0.5835\n",
      "Epoch 22/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3163 - acc: 0.5835\n",
      "Epoch 23/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3119 - acc: 0.5842\n",
      "Epoch 24/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3101 - acc: 0.5864\n",
      "Epoch 25/30\n",
      "149415/149415 [==============================] - 3s 23us/step - loss: 1.3070 - acc: 0.5907\n",
      "Epoch 26/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3061 - acc: 0.6017\n",
      "Epoch 27/30\n",
      "149415/149415 [==============================] - 3s 22us/step - loss: 1.3034 - acc: 0.6049\n",
      "Epoch 28/30\n",
      "149415/149415 [==============================] - 3s 20us/step - loss: 1.3012 - acc: 0.6075\n",
      "Epoch 29/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.3003 - acc: 0.6084\n",
      "Epoch 30/30\n",
      "149415/149415 [==============================] - 3s 21us/step - loss: 1.2983 - acc: 0.6092\n",
      "149415/149415 [==============================] - 11s 72us/step\n",
      "\n",
      "Training Score: 60.94\n",
      "64036/64036 [==============================] - 6s 86us/step\n",
      "\n",
      "Testing Score: 60.95\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 12)                1380      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                156       \n",
      "=================================================================\n",
      "Total params: 1,692\n",
      "Trainable params: 1,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The confusion matrix is : \n",
      " [[    0     0     0     0     0     0     0   109     0     0    53     0]\n",
      " [    0     0     0     0     0     0     0   306     0     0   122     0]\n",
      " [    0     0     0     0     0     0     0   226     0     0    92     0]\n",
      " [    0     0     0     0     0     0     0   476     0     0   199     0]\n",
      " [    0     0     0     0     0     0     0  1085     0     0   422     0]\n",
      " [    0     0     0     0     0     0     0   499     0     0   198     0]\n",
      " [    0     0     0     0     0     0     0   628     0     0   223     0]\n",
      " [    0     0     0     0     0     0     0 33709     0     0  3654     0]\n",
      " [    0     0     0     0     0     0     0   161     0     0    68     0]\n",
      " [    0     0     0     0     0     0     0    48     0     0    17     0]\n",
      " [    0     0     0     0     0     0     0 13395     0     0  5318     0]\n",
      " [    0     0     0     0     0     0     0  2208     0     0   820     0]]\n"
     ]
    }
   ],
   "source": [
    "ANN(df,df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "Repositories\n",
    "\n",
    "https://github.com/karvenka/kaggle-airbnb/blob/master/notebooks/Venkatesan_Karthick_Final_Project_Report.ipynb\n",
    "\n",
    "https://github.com/Sapphirine/Airbnb-New-User-Bookings-Prediction/blob/master/preprocessing%26prediction.ipynb\n",
    "\n",
    "https://github.com/Currie32/AirBnB-Predicting-Destination/blob/master/Predicting_Destination.ipynb\n",
    "\n",
    "Kaggle Competition\n",
    "\n",
    "https://www.kaggle.com/meicher/predicting-first-destination-4-models\n",
    "\n",
    "https://www.kaggle.com/svpons/three-level-classification-architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the document by Kandarp Vyas is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
